{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://jalammar.github.io/illustrated-gpt2/"
      ],
      "metadata": {
        "id": "X0-rbWwZ2ta4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KOPRxCG-2LPc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import * # Input, Embedding, Dense, LayerNormalization, MultiheadAttention, Dropout\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, max_sequence_length, num_layers=12, num_heads=8, d_model=512, dff=1024, dropout_rate=0.0):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, d_model) ## 입력 문자열을 정해진 길이의 벡터로 임베딩, Simple traditional embedding\n",
        "        self.positional_encoding = self.positional_encoding(max_sequence_length, d_model) ## 위치 정보 부여를 위한 positional encoding, RNN 기반 레이어와 달리, Attention 레이어에는 데이터가 병렬로 입력되기 때문.\n",
        "\n",
        "        self.encoder_layer = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)] ## dff is dimension of feed-forward\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "        self.final_layer = Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        sequence_length = tf.shape(inputs)[1]\n",
        "        mask = self.create_padding_mask(inputs)\n",
        "\n",
        "        # Embedding and Positional Encoding\n",
        "        x = self.Embedding(inputs)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.positional_encoding[:, :sequence_length, :] # return [batch, sequence_length, model_dimension]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Encoder Layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoder_layer[i](x, mask)\n",
        "\n",
        "        output = self.final_layer(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def positional_encoding(self, sequence_length, d_model):\n",
        "        position = tf.expand_dims(tf.range(0, sequence_length, dtype=tf.float32), axis=1)\n",
        "        div_term = tf.pow(10000, 2 * tf.range(0, d_model, 2, dtype=tf.float32) / d_model)\n",
        "        pe = tf.concat([tf.sin(position / div_term), tf.cos(position / div_term)], axis=1)\n",
        "        return tf.expand_dims(pe, axis=0)\n",
        "\n",
        "    # 특정 단어의 마스킹을 위함\n",
        "    def create_padding_mask(self, sequence):\n",
        "        mask = tf.cast(tf.math.equal(sequence, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "WJ0_kDvX2ugL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.attention.multi_head_attention import MultiHeadAttention\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # need to attention layer, dropout layer, layernorm layer and feed-forward layer\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.ffn = self.point_wise_feed_forward_network(d_model, dff)\n",
        "        self.dropout2 = Dropoit(dropout_rate)\n",
        "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask):\n",
        "        attention_output = self.mha(inputs, inputs, attention_mask=mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        output1 = self.ln1(inputs + attention_output)\n",
        "\n",
        "        ffn_output = self.ffn(output1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        output2 = self.ln2(output1 + ffn_output)\n",
        "\n",
        "        return output2\n",
        "\n",
        "    def point_wise_feed_forward_network(self, d_model, dff):\n",
        "        return tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])"
      ],
      "metadata": {
        "id": "nWs05AcG1YGg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Example"
      ],
      "metadata": {
        "id": "-HcSHJuaUmbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 12\n",
        "num_heads = 8\n",
        "d_model = 512\n",
        "dff = 1024\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Create an instance of the GPT-2 model\n",
        "gpt2 = GPT2(vocab_size, max_sequence_length, num_layers, num_heads, d_model, dff, dropout_rate)\n",
        "\n",
        "# Compile the model\n",
        "gpt2.compile(optimizer='adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Train the model\n",
        "gpt.fit(train_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "Jl4oWDSJUlv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 with HuggingFace transformers"
      ],
      "metadata": {
        "id": "4uYi2yC_WBwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkPlx-fZWFLc",
        "outputId": "13e0eae8-ad85-49dd-e7f5-a403e7de80c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Generate text using the GPT-2 model\n",
        "def generate_text(input_text, max_length=100):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage:\n",
        "input_sequence = 'Once upon a time'\n",
        "generated_text = generate_text(input_sequence, max_length=100)\n",
        "\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "IjDhcS42WHeY",
        "outputId": "23408f16-7dcb-4763-b5e8-85a9798848ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w5aY8xeVXecD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}